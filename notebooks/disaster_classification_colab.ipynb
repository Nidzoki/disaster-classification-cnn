{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo8dTCRESFQn"
      },
      "source": [
        "# Klasifikacija prirodnih nepogode\n",
        "___\n",
        "<br>\n",
        "\n",
        "Za realizaciju ovoga projekta, korišten je skup podataka - [disaster dataset](https://www.kaggle.com/datasets/sarthaktandulje/disaster-damage-5class). Ovaj skup podataka sadrži slike prirodnih nepogoda:\n",
        "- požar\n",
        "- dim(mogući požar)\n",
        "- poplava\n",
        "- klizište\n",
        "\n",
        "Također, skup sadrži i slike na kojima nisu prikazane navedene nepogode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHU3uEy5UejG"
      },
      "source": [
        "### 1. Uvoz skupa podataka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xtMt-zvjSEpe"
      },
      "outputs": [],
      "source": [
        "from glob import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7chEW6UTUU58",
        "outputId": "3092f838-53a4-40b8-b3fc-3a0e756761ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIbaegcKVxxm"
      },
      "source": [
        "### 1.1 Struktura skupa podataka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-zxYZw1U_6B",
        "outputId": "3eb6ae4e-4e3f-4a9e-d13b-98aa570dfaff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------+--------------+\n",
            "| Direktorij                                        |   Broj slika |\n",
            "+===================================================+==============+\n",
            "| /content/drive/MyDrive/disaster_dataset/smoke     |          302 |\n",
            "+---------------------------------------------------+--------------+\n",
            "| /content/drive/MyDrive/disaster_dataset/normal    |         2226 |\n",
            "+---------------------------------------------------+--------------+\n",
            "| /content/drive/MyDrive/disaster_dataset/fire      |         2537 |\n",
            "+---------------------------------------------------+--------------+\n",
            "| /content/drive/MyDrive/disaster_dataset/flood     |         2706 |\n",
            "+---------------------------------------------------+--------------+\n",
            "| /content/drive/MyDrive/disaster_dataset/landslide |          310 |\n",
            "+---------------------------------------------------+--------------+\n",
            "\n",
            "Ukupan broj slika: 8081\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "directories = glob('/content/drive/MyDrive/disaster_dataset/*')\n",
        "\n",
        "dataset_structure = [[x, len(glob(x + '/*'))] for x in directories]\n",
        "\n",
        "print(tabulate([[x, len(glob(x + '/*'))] for x in directories], headers=['Direktorij', 'Broj slika'], tablefmt=\"grid\"))\n",
        "\n",
        "print(f'\\nUkupan broj slika: {sum([len(glob(x + \"/*\")) for x in directories])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM5dE15ydax9"
      },
      "source": [
        "### 1.2 Odabir uzorka"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS1oZxKX1IUL"
      },
      "source": [
        "Zbog velike neuravnoteženosti skupa podataka, moglo bi doći do pristranosti modela klasama `normal`, `fire` i `flood`. Primjerice, skup slika klase `flood` je oko 9 puta veći od skupova klasa `smoke` i `landslide`.\n",
        "\n",
        "Metode kojima se ovakav tip problema redovito rješava su:\n",
        "\n",
        "<br>\n",
        "\n",
        "| Metoda | Opis |\n",
        "| :--- | :--- |\n",
        "| **Ponderiranje klasa (Class Weighting)** | Dodjeljuju se veće težine (weights) manjinskim klasama <br><br> (Dim: 302, Klizište: 310) u funkciji gubitka (loss function). <br><br> To prisiljava model da pridaje veću važnost **ispravnom** klasificiranju tih rjeđih primjera. <br><br> Ovo je često najjednostavniji i najučinkovitiji pristup. |\n",
        "| **Oversampling** | **Povećavanje** broja primjera u manjinskim klasama <br><br>(npr. tehnikama poput **SMOTE** ili jednostavno ponavljanjem postojećih slika) <br><br>kako bi se izjednačio broj s dominantnim klasama. |\n",
        "| **Undersampling** | **Smanjivanje** broja primjera u dominantnim klasama <br><br>(Požar: 2537, Poplava: 2706) nasumičnim uklanjanjem slika. <br><br>**Oprez:** Gubitak potencijalno važnih informacija je rizik. |\n",
        "| **Augmentacija podataka (Data Augmentation)** | Stvaranje novih primjera iz postojećih manjinskih slika primjenom transformacija <br><br>(npr. rotacija, zrcaljenje, izrezivanje, promjena svjetline) <br><br>kako bi se umjetno povećao njihov broj i raznolikost. |\n",
        "\n",
        "<br>\n",
        "\n",
        "Za potrebe ovoga projekta, iskoristit će se metoda undersamplinga kako bi se smanjila iskorištenost računalnih resursa i skratilo vrijeme učenja modela.\n",
        "To znači da će skupovi `smoke` i `landslide` ostati gotovo isti, a iz klasa `fire`, `flood` i `normal` će se uzeti nasumično odabrani uzorak koji će biti blizu broja podataka u najmanjem skupu.\n",
        "\n",
        "<br>\n",
        "\n",
        "Kako je klasa `smoke`, klasa koja ima najmanje podataka (302), zaokružit ćemo taj broj, te uzeti slučajni uzorak od 300 slika iz svake klase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B59yAssxT-l"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yymYcBL6K1q"
      },
      "outputs": [],
      "source": [
        "np.random.seed(9)\n",
        "\n",
        "fire_sample = np.random.choice(glob(directories[0] + '/*.jpg'), 300, replace=False)\n",
        "flood_sample = np.random.choice(glob(directories[1] + '/*.jpg'), 300, replace=False)\n",
        "normal_sample = np.random.choice(glob(directories[2] + '/*.jpg'), 300, replace=False)\n",
        "smoke_sample = np.random.choice(glob(directories[3] + '/*.jpg'), 300, replace=False)\n",
        "landslide_sample = np.random.choice(glob(directories[4] + '/*.jpg'), 300, replace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcmY_rbxk-cF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "all_paths = np.concatenate([\n",
        "    fire_sample,\n",
        "    flood_sample,\n",
        "    normal_sample,\n",
        "    smoke_sample,\n",
        "    landslide_sample\n",
        "])\n",
        "# 0=fire, 1=flood, 2=normal, 3=smoke, 4=landslide (primjer)\n",
        "labels = np.array(\n",
        "    [0] * 300 +\n",
        "    [1] * 300 +\n",
        "    [2] * 300 +\n",
        "    [3] * 300 +\n",
        "    [4] * 300\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qxaYtwAlJG3",
        "outputId": "20257a9a-5882-42ad-aed6-816383afd993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trening skup (70%): 1050 slika\n",
            "Validacija skup (15%): 225 slika\n",
            "Test skup (15%): 225 slika\n"
          ]
        }
      ],
      "source": [
        "X_train_paths, X_temp_paths, y_train, y_temp = train_test_split(\n",
        "    all_paths, labels, test_size=0.3, random_state=9, stratify=labels\n",
        ")\n",
        "\n",
        "X_val_paths, X_test_paths, y_val, y_test = train_test_split(\n",
        "    X_temp_paths, y_temp, test_size=0.5, random_state=9, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"Trening skup (70%): {len(X_train_paths)} slika\") # 1050\n",
        "print(f\"Validacija skup (15%): {len(X_val_paths)} slika\") # 225\n",
        "print(f\"Test skup (15%): {len(X_test_paths)} slika\") # 225"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCkaoKhb3kYB",
        "outputId": "75368f9a-52b8-44c8-9203-cb169c2dc8c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Kopiranje fajlova s Drive-a na lokalni disk Colaba...\n",
            "\n",
            "Kopiranje uspješno završeno. Inicijalizacija generatora...\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python\n",
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import tensorflow as tf\n",
        "\n",
        "LOCAL_BASE_DIR = '/tmp/disaster_data_local/'\n",
        "os.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n",
        "LOCAL_TRAIN_DIR = os.path.join(LOCAL_BASE_DIR, 'train')\n",
        "LOCAL_VAL_DIR = os.path.join(LOCAL_BASE_DIR, 'val')\n",
        "LOCAL_TEST_DIR = os.path.join(LOCAL_BASE_DIR, 'test')\n",
        "\n",
        "class_names = ['fire', 'flood', 'normal', 'smoke', 'landslide']\n",
        "for name in class_names:\n",
        "    os.makedirs(os.path.join(LOCAL_TRAIN_DIR, name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(LOCAL_VAL_DIR, name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(LOCAL_TEST_DIR, name), exist_ok=True)\n",
        "\n",
        "def copy_and_update_paths(old_paths, labels, local_base_dir):\n",
        "    new_paths = []\n",
        "    for old_path, label in zip(old_paths, labels):\n",
        "        class_name = class_names[label]\n",
        "        local_class_dir = os.path.join(local_base_dir, class_name)\n",
        "        file_name = os.path.basename(old_path)\n",
        "        new_path = os.path.join(local_class_dir, file_name)\n",
        "        shutil.copy2(old_path, new_path)\n",
        "        new_paths.append(new_path)\n",
        "    return np.array(new_paths)\n",
        "\n",
        "print(\"Kopiranje fajlova s Drive-a na lokalni disk Colaba...\")\n",
        "\n",
        "X_train_local = copy_and_update_paths(X_train_paths, y_train, LOCAL_TRAIN_DIR)\n",
        "X_val_local = copy_and_update_paths(X_val_paths, y_val, LOCAL_VAL_DIR)\n",
        "X_test_local = copy_and_update_paths(X_test_paths, y_test, LOCAL_TEST_DIR)\n",
        "\n",
        "print(\"\\nKopiranje uspješno završeno. Inicijalizacija generatora...\")\n",
        "\n",
        "\n",
        "class CustomDataGenerator(Sequence):\n",
        "    def __init__(self, image_paths, labels, batch_size, target_size=(224, 224), num_classes=5, shuffle=True):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.num_classes = num_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.image_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        batch_paths = [self.image_paths[k] for k in indices]\n",
        "        batch_labels = [self.labels[k] for k in indices]\n",
        "\n",
        "        X = np.empty((self.batch_size, *self.target_size, 3), dtype=np.float32)\n",
        "\n",
        "        for i, path in enumerate(batch_paths):\n",
        "\n",
        "            img_cv2 = cv2.imread(path)\n",
        "\n",
        "            if img_cv2 is None:\n",
        "                print(f\"Upozorenje: OpenCV nije uspio učitati sliku na lokalnom disku: {path}\")\n",
        "                continue\n",
        "\n",
        "            # Konverzija iz BGR (OpenCV standard) u RGB (Keras/TensorFlow standard)\n",
        "            img_rgb = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            img_resized = cv2.resize(img_rgb, self.target_size)\n",
        "\n",
        "            img_array = img_resized.astype('float32') / 255.0\n",
        "\n",
        "            X[i,] = img_array\n",
        "\n",
        "        # One-hot encoding\n",
        "        y = tf.keras.utils.to_categorical(batch_labels, num_classes=self.num_classes)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indices = np.arange(len(self.image_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "TARGET_SIZE = (224, 224)\n",
        "\n",
        "train_generator = CustomDataGenerator(X_train_local, y_train, BATCH_SIZE, TARGET_SIZE, shuffle=True)\n",
        "validation_generator = CustomDataGenerator(X_val_local, y_val, BATCH_SIZE, TARGET_SIZE, shuffle=False)\n",
        "test_generator = CustomDataGenerator(X_test_local, y_test, BATCH_SIZE, TARGET_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUGFcKTVzHu1",
        "outputId": "40b2df41-3011-405d-ea3f-8d9aebc16009"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m842s\u001b[0m 26s/step - accuracy: 0.3462 - loss: 1.6765 - val_accuracy: 0.7232 - val_loss: 0.8117\n",
            "Epoch 2/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m800s\u001b[0m 25s/step - accuracy: 0.7650 - loss: 0.7289 - val_accuracy: 0.7812 - val_loss: 0.6175\n",
            "Epoch 3/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m792s\u001b[0m 25s/step - accuracy: 0.8318 - loss: 0.4909 - val_accuracy: 0.8438 - val_loss: 0.4957\n",
            "Epoch 4/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m797s\u001b[0m 25s/step - accuracy: 0.8710 - loss: 0.3793 - val_accuracy: 0.8170 - val_loss: 0.4705\n",
            "Epoch 5/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m859s\u001b[0m 27s/step - accuracy: 0.9430 - loss: 0.2547 - val_accuracy: 0.8482 - val_loss: 0.4406\n",
            "Epoch 6/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m817s\u001b[0m 26s/step - accuracy: 0.9457 - loss: 0.2106 - val_accuracy: 0.8482 - val_loss: 0.4175\n",
            "Epoch 7/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m798s\u001b[0m 25s/step - accuracy: 0.9602 - loss: 0.1731 - val_accuracy: 0.8304 - val_loss: 0.4726\n",
            "Epoch 8/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m803s\u001b[0m 25s/step - accuracy: 0.9769 - loss: 0.1252 - val_accuracy: 0.8571 - val_loss: 0.3912\n",
            "Epoch 9/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m794s\u001b[0m 25s/step - accuracy: 0.9869 - loss: 0.1075 - val_accuracy: 0.8750 - val_loss: 0.4307\n",
            "Epoch 10/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m795s\u001b[0m 25s/step - accuracy: 0.9872 - loss: 0.0970 - val_accuracy: 0.8527 - val_loss: 0.4104\n",
            "Epoch 11/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m805s\u001b[0m 25s/step - accuracy: 0.9869 - loss: 0.0908 - val_accuracy: 0.8616 - val_loss: 0.4008\n",
            "Epoch 12/20\n",
            "\u001b[1m 4/32\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:29\u001b[0m 20s/step - accuracy: 0.9896 - loss: 0.0677 "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "NUM_CLASSES = 5\n",
        "\n",
        "base_model = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 3)\n",
        ")\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    validation_data=validation_generator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIMtQh4l0Ba6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f\"Testna točnost: {accuracy*100:.2f}%\")\n",
        "\n",
        "y_pred_probs = model.predict(test_generator)\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "y_true_one_hot = []\n",
        "for i in range(len(test_generator)):\n",
        "    _, batch_y = test_generator[i]\n",
        "    y_true_one_hot.extend(batch_y)\n",
        "y_true = np.argmax(y_true_one_hot, axis=1)\n",
        "\n",
        "\n",
        "target_names = ['fire', 'flood', 'normal', 'smoke', 'landslide']\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_true, y_pred_classes, target_names=target_names))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Matrica konfuzije')\n",
        "plt.ylabel('Stvarna klasa')\n",
        "plt.xlabel('Predviđena klasa')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
